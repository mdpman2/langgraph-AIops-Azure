# Copyright (c) Microsoft. All rights reserved.
"""
LangGraph-style Workflow Implementation using Azure OpenAI

Planning â†’ Execution â†’ Reflection â†’ Decision ì‚¬ì´í´ì„ êµ¬í˜„í•œ ì›Œí¬í”Œë¡œìš°
Azure OpenAI SDKë¥¼ ì§ì ‘ ì‚¬ìš©
"""

import asyncio
import json
import structlog
from datetime import datetime, timezone
from functools import lru_cache
from typing import Any, Dict, List, Optional, TypeVar
from uuid import uuid4

from openai import AsyncAzureOpenAI
from azure.identity.aio import DefaultAzureCredential, get_bearer_token_provider

from .models import (
    AgentState,
    Plan,
    PlanStep,
    ExecutionResult,
    ReflectionResult,
    Decision,
    DecisionType,
    WorkflowStage,
)
from .config import AgentConfig, load_config

logger = structlog.get_logger(__name__)

# Type variable for generic typing
T = TypeVar('T', bound=Dict[str, Any])


def _extract_json_from_text(text: str, default: Optional[T] = None) -> T:
    """í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ - ìµœì í™”ëœ íŒŒì‹±"""
    if not text:
        return default or {}

    try:
        # JSON ë¸”ë¡ ì¶”ì¶œ (í•œ ë²ˆë§Œ ê²€ìƒ‰)
        json_start = text.find("{")
        if json_start == -1:
            return default or {}

        json_end = text.rfind("}") + 1
        if json_end <= json_start:
            return default or {}

        return json.loads(text[json_start:json_end])
    except json.JSONDecodeError:
        return default or {}


@lru_cache(maxsize=10)
def _get_system_prompt(prompt_type: str) -> str:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìºì‹±"""
    prompts = {
        "planning": """ë‹¹ì‹ ì€ ì „ë¬¸ ê³„íš ìˆ˜ë¦½ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹ (JSON):
{
    "goal": "ìµœì¢… ëª©í‘œ",
    "steps": [
        {
            "step_number": 1,
            "description": "ë‹¨ê³„ ì„¤ëª…",
            "action": "ìˆ˜í–‰í•  ì•¡ì…˜",
            "expected_output": "ì˜ˆìƒ ì¶œë ¥"
        }
    ]
}

ê·œì¹™:
1. ê° ë‹¨ê³„ëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤
2. ë‹¨ê³„ ê°„ ì˜ì¡´ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”
3. ìµœëŒ€ 5ë‹¨ê³„ë¡œ ì œí•œí•˜ì„¸ìš”
4. ì¬ê³„íš ì‹œ ì´ì „ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì„¸ìš”""",

        "execution": """ë‹¹ì‹ ì€ ì‹¤í–‰ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”.""",

        "reflection": """ë‹¹ì‹ ì€ ë¹„íŒì  í‰ê°€ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹ (JSON):
{
    "quality_score": 0.0-1.0,
    "goal_alignment": 0.0-1.0,
    "completeness": 0.0-1.0,
    "strengths": ["ì˜ëœ ì ë“¤"],
    "weaknesses": ["ê°œì„  í•„ìš” ì ë“¤"],
    "suggestions": ["ê°œì„  ì œì•ˆë“¤"],
    "needs_retry": true/false,
    "needs_replan": true/false,
    "reasoning": "íŒë‹¨ ê·¼ê±°"
}

í‰ê°€ ê¸°ì¤€:
1. ëª©í‘œì™€ì˜ ì¼ì¹˜ë„
2. ê²°ê³¼ì˜ ì™„ì„±ë„
3. í’ˆì§ˆ ìˆ˜ì¤€
4. ì‹¤í–‰ íš¨ìœ¨ì„±"""
    }
    return prompts.get(prompt_type, "")


# GPT-5.x ëª¨ë¸ ê°ì§€ë¥¼ ìœ„í•œ frozenset (O(1) ì„±ëŠ¥)
GPT5_MODELS = frozenset({"gpt-5", "gpt-5.1", "gpt-5.2", "gpt-5-mini", "gpt-5-nano", "gpt-5-pro", "model-router"})
REASONING_MODELS = frozenset({"gpt-5", "gpt-5.1", "gpt-5.2", "o1", "o3", "o3-mini", "o4-mini", "model-router"})


def _is_gpt5_model(model_name: str) -> bool:
    """ëª¨ë¸ì´ GPT-5.x ì‹œë¦¬ì¦ˆì¸ì§€ í™•ì¸ - O(1) ìµœì í™”"""
    if not model_name:
        return False
    model_lower = model_name.lower()
    # 1. ì§ì ‘ ë§¤ì¹­ (O(1) frozenset ì¡°íšŒ)
    if model_lower in GPT5_MODELS:
        return True
    # 2. ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ (deployment nameì— ëª¨ë¸ëª… í¬í•¨ëœ ê²½ìš°)
    return any(m in model_lower for m in GPT5_MODELS)


class AzureOpenAIClient:
    """Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ë˜í¼ - GPT-5.x ì§€ì› (2026-01 ì—…ë°ì´íŠ¸)

    ìµœì í™”:
    - ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§€ì›
    - ì‹±ê¸€í†¤ í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬
    - ìë™ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    """

    __slots__ = ('config', '_client', '_credential')

    def __init__(self, config: AgentConfig):
        self.config = config
        self._client: Optional[AsyncAzureOpenAI] = None
        self._credential: Optional[DefaultAzureCredential] = None

    async def __aenter__(self) -> "AzureOpenAIClient":
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ - ìë™ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.close()

    async def get_client(self) -> AsyncAzureOpenAI:
        """Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
        if self._client is None:
            # API ë²„ì „: configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
            api_version = getattr(self.config, 'azure_openai_api_version', '2024-12-01-preview')

            if self.config.azure_openai_api_key:
                # API í‚¤ ì‚¬ìš©
                self._client = AsyncAzureOpenAI(
                    api_key=self.config.azure_openai_api_key,
                    azure_endpoint=self.config.azure_openai_endpoint,
                    api_version=api_version,
                )
            else:
                # DefaultAzureCredential ì‚¬ìš©
                self._credential = DefaultAzureCredential()
                token_provider = get_bearer_token_provider(
                    self._credential,
                    "https://cognitiveservices.azure.com/.default"
                )
                self._client = AsyncAzureOpenAI(
                    azure_ad_token_provider=token_provider,
                    azure_endpoint=self.config.azure_openai_endpoint or self.config.azure_foundry_project_endpoint,
                    api_version=api_version,
                )
        return self._client

    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        reasoning_effort: Optional[str] = None,
    ) -> str:
        """ì±„íŒ… ì™„ì„± ìš”ì²­ - GPT-5.x íŒŒë¼ë¯¸í„° ì§€ì›"""
        client = await self.get_client()
        deployment = self.config.azure_openai_deployment_name or self.config.azure_foundry_model_deployment

        # GPT-5.x ëª¨ë¸ ê°ì§€
        is_gpt5 = _is_gpt5_model(deployment)

        # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        params = {
            "model": deployment,
            "messages": messages,
            "temperature": temperature,
        }

        # GPT-5.x ì „ìš© íŒŒë¼ë¯¸í„°
        if is_gpt5:
            params["max_completion_tokens"] = max_tokens
            # reasoning_effort ì„¤ì • (ì œê³µëœ ê²½ìš° ë˜ëŠ” configì—ì„œ)
            effort = reasoning_effort or getattr(self.config, 'reasoning_effort', 'medium')
            if effort and effort != 'none':
                params["reasoning_effort"] = effort
            logger.info("gpt5_request", model=deployment, reasoning_effort=effort)
        else:
            params["max_tokens"] = max_tokens

        response = await client.chat.completions.create(**params)

        return response.choices[0].message.content or ""

    async def chat_with_structured_output(
        self,
        messages: List[Dict[str, str]],
        response_schema: Dict[str, Any],
        temperature: float = 0.3,
        max_tokens: int = 2000,
    ) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ ìš”ì²­ - Structured Outputs (2026 ìµœì‹ )"""
        client = await self.get_client()
        deployment = self.config.azure_openai_deployment_name or self.config.azure_foundry_model_deployment
        is_gpt5 = _is_gpt5_model(deployment)

        params = {
            "model": deployment,
            "messages": messages,
            "temperature": temperature,
            "response_format": {
                "type": "json_schema",
                "json_schema": {
                    "name": "structured_response",
                    "strict": True,
                    "schema": response_schema
                }
            }
        }

        if is_gpt5:
            params["max_completion_tokens"] = max_tokens
        else:
            params["max_tokens"] = max_tokens

        response = await client.chat.completions.create(**params)
        content = response.choices[0].message.content or "{}"

        try:
            return json.loads(content)
        except json.JSONDecodeError:
            return _extract_json_from_text(content, {})

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._client:
            await self._client.close()
        if self._credential:
            await self._credential.close()


# ============================================
# Structured Output Schemas (2026 ìµœì‹ )
# ============================================

PLAN_SCHEMA = {
    "type": "object",
    "properties": {
        "goal": {"type": "string", "description": "ìµœì¢… ëª©í‘œ"},
        "steps": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "step_number": {"type": "integer"},
                    "description": {"type": "string"},
                    "action": {"type": "string"},
                    "expected_output": {"type": "string"}
                },
                "required": ["step_number", "description", "action", "expected_output"],
                "additionalProperties": False
            }
        }
    },
    "required": ["goal", "steps"],
    "additionalProperties": False
}

REFLECTION_SCHEMA = {
    "type": "object",
    "properties": {
        "quality_score": {"type": "number", "minimum": 0, "maximum": 1},
        "goal_alignment": {"type": "number", "minimum": 0, "maximum": 1},
        "completeness": {"type": "number", "minimum": 0, "maximum": 1},
        "strengths": {"type": "array", "items": {"type": "string"}},
        "weaknesses": {"type": "array", "items": {"type": "string"}},
        "suggestions": {"type": "array", "items": {"type": "string"}},
        "needs_retry": {"type": "boolean"},
        "needs_replan": {"type": "boolean"},
        "reasoning": {"type": "string"}
    },
    "required": ["quality_score", "goal_alignment", "completeness", "needs_retry", "needs_replan", "reasoning"],
    "additionalProperties": False
}


# ============================================
# Planning Node - ê³„íš ìˆ˜ë¦½ (Structured Outputs ì§€ì›)
# ============================================

async def planning_node(
    client: AzureOpenAIClient,
    state: AgentState,
    feedback: Optional[str] = None,
    use_structured_output: bool = True
) -> AgentState:
    """ê³„íš ìˆ˜ë¦½ ë…¸ë“œ - Structured Outputs ì§€ì› (2026 ìµœì‹ )"""
    logger.info("planning_started", session_id=state.session_id, feedback=feedback, structured=use_structured_output)

    system_prompt = _get_system_prompt("planning")
    user_prompt = f"ì‚¬ìš©ì ìš”ì²­: {state.user_request}"
    if feedback:
        user_prompt = f"{user_prompt}\n\nì´ì „ ì‹œë„ì˜ í”¼ë“œë°±: {feedback}"

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # Structured Outputs ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
    if use_structured_output and getattr(client.config, 'use_structured_outputs', True):
        try:
            plan_data = await client.chat_with_structured_output(
                messages=messages,
                response_schema=PLAN_SCHEMA,
                temperature=0.5,
            )
            logger.info("structured_output_success", node="planning")
        except Exception as e:
            logger.warning("structured_output_fallback", error=str(e))
            response_text = await client.chat(messages, reasoning_effort="medium")
            plan_data = _extract_json_from_text(response_text, {"goal": state.user_request, "steps": []})
    else:
        response_text = await client.chat(messages, reasoning_effort="medium")
        plan_data = _extract_json_from_text(response_text, {"goal": state.user_request, "steps": []})

    # Plan ê°ì²´ ìƒì„±
    steps = []
    for step_data in plan_data.get("steps", []):
        steps.append(PlanStep(
            step_number=step_data.get("step_number", len(steps) + 1),
            description=step_data.get("description", ""),
            action=step_data.get("action", ""),
            expected_output=step_data.get("expected_output", ""),
        ))

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    version = (state.current_plan.version + 1) if state.current_plan else 1
    state.current_plan = Plan(
        goal=plan_data.get("goal", state.user_request),
        steps=steps,
        version=version,
    )
    state.current_stage = WorkflowStage.EXECUTION
    state.add_message("assistant", f"ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {len(steps)}ê°œ ë‹¨ê³„")

    logger.info("planning_completed",
               session_id=state.session_id,
               plan_version=version,
               steps_count=len(steps))

    return state


# ============================================
# Execution Node - ì‹¤í–‰
# ============================================

async def execution_node(client: AzureOpenAIClient, state: AgentState) -> AgentState:
    """ì‹¤í–‰ ë…¸ë“œ"""
    logger.info("execution_started", session_id=state.session_id)

    if not state.current_plan:
        state.error_message = "ì‹¤í–‰í•  ê³„íšì´ ì—†ìŠµë‹ˆë‹¤"
        state.current_stage = WorkflowStage.FAILED
        return state

    # ë‹¤ìŒ ì‹¤í–‰í•  ë‹¨ê³„ ê°€ì ¸ì˜¤ê¸°
    next_step = state.current_plan.get_next_step()
    if not next_step:
        state.current_stage = WorkflowStage.REFLECTION
        return state

    next_step.status = "in_progress"
    start_time = datetime.now(timezone.utc)

    system_prompt = _get_system_prompt("execution")
    user_prompt = f"""
ì‘ì—…: {next_step.description}
ì•¡ì…˜: {next_step.action}
ì˜ˆìƒ ì¶œë ¥: {next_step.expected_output}

ì „ì²´ ëª©í‘œ: {state.current_plan.goal}
ì‚¬ìš©ì ìš”ì²­: {state.user_request}

ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    try:
        response_text = await client.chat(messages)
        end_time = datetime.now(timezone.utc)
        duration = (end_time - start_time).total_seconds()

        execution_result = ExecutionResult(
            step_id=next_step.step_id,
            success=True,
            output=response_text,
            duration_seconds=duration,
        )
        state.execution_history.append(execution_result)
        next_step.status = "completed"
        state.add_message("assistant", f"ë‹¨ê³„ {next_step.step_number} ì‹¤í–‰ ì™„ë£Œ")

        logger.info("execution_step_completed",
                   session_id=state.session_id,
                   step_id=next_step.step_id,
                   duration=duration)

    except Exception as e:
        end_time = datetime.now(timezone.utc)
        duration = (end_time - start_time).total_seconds()

        execution_result = ExecutionResult(
            step_id=next_step.step_id,
            success=False,
            output=None,
            error=str(e),
            duration_seconds=duration,
        )
        state.execution_history.append(execution_result)
        next_step.status = "failed"

        logger.error("execution_step_failed",
                    session_id=state.session_id,
                    step_id=next_step.step_id,
                    error=str(e))

    # ë‹¤ìŒ ë‹¨ê³„ í™•ì¸
    if state.current_plan.is_complete():
        state.current_stage = WorkflowStage.REFLECTION
    else:
        state.current_stage = WorkflowStage.EXECUTION

    state.iteration_count += 1
    return state


# ============================================
# Reflection Node - ì„±ì°° (Structured Outputs ì§€ì›)
# ============================================

async def reflection_node(
    client: AzureOpenAIClient,
    state: AgentState,
    use_structured_output: bool = True
) -> AgentState:
    """ì„±ì°° ë…¸ë“œ - Structured Outputs ë° ê³ ê¸‰ ì¶”ë¡  ì§€ì› (2026 ìµœì‹ )"""
    logger.info("reflection_started", session_id=state.session_id, structured=use_structured_output)

    recent_executions = state.execution_history[-5:] if state.execution_history else []
    system_prompt = _get_system_prompt("reflection")

    execution_summary = "\n".join([
        f"- ë‹¨ê³„ {e.step_id}: {'ì„±ê³µ' if e.success else 'ì‹¤íŒ¨'}, ì¶œë ¥: {str(e.output)[:200]}"
        for e in recent_executions
    ])

    user_prompt = f"""
ì›ë˜ ëª©í‘œ: {state.current_plan.goal if state.current_plan else state.user_request}
ì‚¬ìš©ì ìš”ì²­: {state.user_request}

ì‹¤í–‰ ê²°ê³¼:
{execution_summary}

ì´ ê²°ê³¼ë“¤ì„ í‰ê°€í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    default_reflection = {
        "quality_score": 0.5,
        "goal_alignment": 0.5,
        "completeness": 0.5,
        "strengths": [],
        "weaknesses": [],
        "suggestions": [],
        "needs_retry": False,
        "needs_replan": False,
        "reasoning": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"
    }

    # Structured Outputs ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
    if use_structured_output and getattr(client.config, 'use_structured_outputs', True):
        try:
            reflection_data = await client.chat_with_structured_output(
                messages=messages,
                response_schema=REFLECTION_SCHEMA,
                temperature=0.3,  # í‰ê°€ëŠ” ë‚®ì€ temperature
            )
            logger.info("structured_output_success", node="reflection")
        except Exception as e:
            logger.warning("structured_output_fallback", node="reflection", error=str(e))
            # Fallback: ê³ ê¸‰ ì¶”ë¡  ì‚¬ìš© (reasoning_effort=high)
            response_text = await client.chat(messages, reasoning_effort="high")
            reflection_data = _extract_json_from_text(response_text, default_reflection)
    else:
        # ê¸°ì¡´ ë°©ì‹ + ê³ ê¸‰ ì¶”ë¡ 
        response_text = await client.chat(messages, reasoning_effort="high")
        reflection_data = _extract_json_from_text(response_text, default_reflection)

    latest_execution = state.get_latest_execution()
    reflection_result = ReflectionResult(
        execution_id=latest_execution.execution_id if latest_execution else "",
        quality_score=reflection_data.get("quality_score", 0.5),
        goal_alignment=reflection_data.get("goal_alignment", 0.5),
        completeness=reflection_data.get("completeness", 0.5),
        strengths=reflection_data.get("strengths", []),
        weaknesses=reflection_data.get("weaknesses", []),
        suggestions=reflection_data.get("suggestions", []),
        needs_retry=reflection_data.get("needs_retry", False),
        needs_replan=reflection_data.get("needs_replan", False),
        reasoning=reflection_data.get("reasoning", ""),
    )

    state.reflection_history.append(reflection_result)
    state.reflection_count += 1
    state.current_stage = WorkflowStage.DECISION
    state.add_message("assistant", f"ì„±ì°° ì™„ë£Œ: í’ˆì§ˆ ì ìˆ˜ {reflection_result.quality_score:.2f}")

    logger.info("reflection_completed",
               session_id=state.session_id,
               quality_score=reflection_result.quality_score,
               needs_retry=reflection_result.needs_retry,
               needs_replan=reflection_result.needs_replan)

    return state


# ============================================
# Decision Node - ê²°ì •
# ============================================

def decision_node(config: AgentConfig, state: AgentState) -> tuple[AgentState, Decision]:
    """ê²°ì • ë…¸ë“œ"""
    logger.info("decision_started", session_id=state.session_id)

    latest_reflection = state.get_latest_reflection()

    if not latest_reflection:
        decision = Decision(
            decision_type=DecisionType.FAIL,
            reasoning="ì„±ì°° ê²°ê³¼ê°€ ì—†ìŒ",
            next_action="fail",
            confidence=1.0,
        )
        state.decision_history.append(decision)
        state.current_stage = WorkflowStage.FAILED
        state.error_message = "ì„±ì°° ê²°ê³¼ê°€ ì—†ì–´ ì§„í–‰ ë¶ˆê°€"
        return state, decision

    # ê²°ì • ë¡œì§
    if latest_reflection.quality_score >= 0.8 and latest_reflection.completeness >= 0.8:
        decision_type = DecisionType.COMPLETE
        reasoning = f"í’ˆì§ˆ ì ìˆ˜ {latest_reflection.quality_score:.2f}, ì™„ì„±ë„ {latest_reflection.completeness:.2f}ë¡œ ëª©í‘œ ë‹¬ì„±"
        next_action = "complete"
        confidence = 0.9

    elif latest_reflection.needs_replan and state.reflection_count < config.max_reflection_iterations:
        decision_type = DecisionType.REPLAN
        reasoning = f"ì„±ì°° ê²°ê³¼ ì¬ê³„íš í•„ìš”: {latest_reflection.reasoning}"
        next_action = "replan"
        confidence = 0.7

    elif latest_reflection.needs_retry and state.reflection_count < config.max_reflection_iterations:
        decision_type = DecisionType.RETRY
        reasoning = f"ì„±ì°° ê²°ê³¼ ì¬ì‹œë„ í•„ìš”: {latest_reflection.reasoning}"
        next_action = "retry"
        confidence = 0.7

    elif state.reflection_count >= config.max_reflection_iterations:
        if latest_reflection.quality_score >= 0.5:
            decision_type = DecisionType.COMPLETE
            reasoning = f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬, í˜„ì¬ í’ˆì§ˆ {latest_reflection.quality_score:.2f}ë¡œ ì™„ë£Œ ì²˜ë¦¬"
            next_action = "complete"
            confidence = 0.6
        else:
            decision_type = DecisionType.FAIL
            reasoning = "ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬, í’ˆì§ˆ ë¯¸ë‹¬ë¡œ ì‹¤íŒ¨ ì²˜ë¦¬"
            next_action = "fail"
            confidence = 0.8
    else:
        decision_type = DecisionType.CONTINUE
        reasoning = "ì¶”ê°€ ì‘ì—…ì´ í•„ìš”í•¨"
        next_action = "continue"
        confidence = 0.7

    decision = Decision(
        decision_type=decision_type,
        reasoning=reasoning,
        next_action=next_action,
        confidence=confidence,
        feedback_for_planning="\n".join(latest_reflection.suggestions) if decision_type == DecisionType.REPLAN else None,
        feedback_for_execution="\n".join(latest_reflection.weaknesses) if decision_type == DecisionType.RETRY else None,
    )

    state.decision_history.append(decision)
    state.add_message("assistant", f"ê²°ì •: {decision_type.value} (ì‹ ë¢°ë„: {confidence:.2f})")

    logger.info("decision_completed",
               session_id=state.session_id,
               decision_type=decision_type.value,
               confidence=confidence)

    return state, decision


def _handle_decision_result(
    state: AgentState,
    decision: Decision,
) -> tuple[AgentState, Optional[str]]:
    """ê²°ì • ê²°ê³¼ì— ë”°ë¥¸ ìƒíƒœ ì—…ë°ì´íŠ¸ - ê³µí†µ ë¡œì§ ì¶”ì¶œ"""
    feedback = None

    if decision.decision_type == DecisionType.COMPLETE:
        state.current_stage = WorkflowStage.COMPLETED
        state.final_output = _generate_final_output(state)
    elif decision.decision_type == DecisionType.FAIL:
        state.current_stage = WorkflowStage.FAILED
    elif decision.decision_type == DecisionType.REPLAN:
        state.current_stage = WorkflowStage.PLANNING
        feedback = decision.feedback_for_planning
    elif decision.decision_type == DecisionType.RETRY:
        if state.current_plan:
            for step in reversed(state.current_plan.steps):
                if step.status == "failed":
                    step.status = "pending"
                    break
        state.current_stage = WorkflowStage.EXECUTION
    else:  # CONTINUE
        state.current_stage = WorkflowStage.EXECUTION

    return state, feedback


def _generate_final_output(state: AgentState) -> str:
    """ìµœì¢… ì¶œë ¥ ìƒì„±"""
    outputs = []
    outputs.append(f"## ì‘ì—… ì™„ë£Œ\n")
    outputs.append(f"**ì›ë˜ ìš”ì²­:** {state.user_request}\n")

    if state.current_plan:
        outputs.append(f"\n### ìˆ˜í–‰ëœ ê³„íš")
        outputs.append(f"**ëª©í‘œ:** {state.current_plan.goal}\n")
        for step in state.current_plan.steps:
            status_icon = "âœ…" if step.status == "completed" else "âŒ"
            outputs.append(f"{status_icon} ë‹¨ê³„ {step.step_number}: {step.description}")

    if state.execution_history:
        outputs.append(f"\n### ì‹¤í–‰ ê²°ê³¼")
        for i, execution in enumerate(state.execution_history[-3:], 1):
            outputs.append(f"\n**ì‹¤í–‰ {i}:**")
            outputs.append(str(execution.output)[:500])

    if state.reflection_history:
        latest = state.reflection_history[-1]
        outputs.append(f"\n### ìµœì¢… í‰ê°€")
        outputs.append(f"- í’ˆì§ˆ ì ìˆ˜: {latest.quality_score:.2f}")
        outputs.append(f"- ëª©í‘œ ì •ë ¬ë„: {latest.goal_alignment:.2f}")
        outputs.append(f"- ì™„ì„±ë„: {latest.completeness:.2f}")

    return "\n".join(outputs)


# ============================================
# Agent Workflow - ì „ì²´ ì›Œí¬í”Œë¡œìš° ì¡°í•©
# ============================================

class AgentWorkflow:
    """
    LangGraph ìŠ¤íƒ€ì¼ AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°

    Planning â†’ Execution â†’ Reflection â†’ Decision ì‚¬ì´í´ êµ¬í˜„
    Azure OpenAI SDK ì§ì ‘ ì‚¬ìš©
    """

    def __init__(self, config: Optional[AgentConfig] = None):
        self.config = config or load_config()
        self._client: Optional[AzureOpenAIClient] = None

    async def _get_client(self) -> AzureOpenAIClient:
        """í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        if self._client is None:
            self._client = AzureOpenAIClient(self.config)
        return self._client

    async def run_stream(self, user_request: str):
        """ìŠ¤íŠ¸ë¦¬ë° ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ - ê° ë‹¨ê³„ë³„ë¡œ ì§„í–‰ ìƒí™©ì„ yield"""
        client = await self._get_client()

        # ì´ˆê¸° ìƒíƒœ ìƒì„±
        state = AgentState(user_request=user_request)

        logger.info("workflow_started",
                   session_id=state.session_id,
                   user_request=user_request[:100])

        yield {"type": "start", "message": "ì›Œí¬í”Œë¡œìš° ì‹œì‘", "session_id": state.session_id}

        feedback: Optional[str] = None

        while state.should_continue():
            current_stage = state.current_stage

            if current_stage == WorkflowStage.PLANNING:
                yield {"type": "stage", "stage": "planning", "message": "ğŸ“‹ ê³„íš ìˆ˜ë¦½ ì¤‘..."}
                state = await planning_node(client, state, feedback)
                feedback = None

                if state.current_plan:
                    plan_info = {
                        "type": "plan",
                        "goal": state.current_plan.goal,
                        "steps": [
                            {"step": s.step_number, "description": s.description}
                            for s in state.current_plan.steps
                        ]
                    }
                    yield plan_info

            elif current_stage == WorkflowStage.EXECUTION:
                yield {"type": "stage", "stage": "execution", "message": "âš¡ ì‹¤í–‰ ì¤‘..."}

                # ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰
                while state.current_stage == WorkflowStage.EXECUTION:
                    if state.current_plan:
                        next_step = state.current_plan.get_next_step()
                        if next_step:
                            yield {
                                "type": "step",
                                "step_number": next_step.step_number,
                                "description": next_step.description,
                                "status": "executing"
                            }

                    state = await execution_node(client, state)

                    # ì‹¤í–‰ ê²°ê³¼ ì „ì†¡
                    if state.execution_history:
                        latest = state.execution_history[-1]
                        yield {
                            "type": "execution_result",
                            "step_id": latest.step_id,
                            "success": latest.success,
                            "output": str(latest.output)[:500] if latest.output else None
                        }

            elif current_stage == WorkflowStage.REFLECTION:
                yield {"type": "stage", "stage": "reflection", "message": "ğŸ” ê²°ê³¼ ë¶„ì„ ì¤‘..."}
                state = await reflection_node(client, state)

                if state.reflection_history:
                    latest = state.reflection_history[-1]
                    yield {
                        "type": "reflection",
                        "quality_score": latest.quality_score,
                        "goal_alignment": latest.goal_alignment,
                        "completeness": latest.completeness
                    }

            elif current_stage == WorkflowStage.DECISION:
                yield {"type": "stage", "stage": "decision", "message": "ğŸ¯ ê²°ì • ì¤‘..."}
                state, decision = decision_node(self.config, state)

                yield {
                    "type": "decision",
                    "decision_type": decision.decision_type.value,
                    "reasoning": decision.reasoning
                }

                state, feedback = _handle_decision_result(state, decision)

                if decision.decision_type in (DecisionType.COMPLETE, DecisionType.FAIL):
                    break

        # ìµœì¢… ê²°ê³¼
        if state.final_output:
            yield {"type": "complete", "result": state.final_output}
        elif state.error_message:
            yield {"type": "error", "message": state.error_message}
        else:
            yield {"type": "complete", "result": "ì›Œí¬í”Œë¡œìš°ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ì¶œë ¥ì´ ì—†ìŠµë‹ˆë‹¤."}

        logger.info("workflow_completed",
                   session_id=state.session_id,
                   final_stage=state.current_stage.value)

    async def run(self, user_request: str) -> str:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        client = await self._get_client()

        # ì´ˆê¸° ìƒíƒœ ìƒì„±
        state = AgentState(user_request=user_request)

        logger.info("workflow_started",
                   session_id=state.session_id,
                   user_request=user_request[:100])

        feedback: Optional[str] = None

        while state.should_continue():
            current_stage = state.current_stage

            if current_stage == WorkflowStage.PLANNING:
                state = await planning_node(client, state, feedback)
                feedback = None

            elif current_stage == WorkflowStage.EXECUTION:
                # ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰
                while state.current_stage == WorkflowStage.EXECUTION:
                    state = await execution_node(client, state)

            elif current_stage == WorkflowStage.REFLECTION:
                state = await reflection_node(client, state)

            elif current_stage == WorkflowStage.DECISION:
                state, decision = decision_node(self.config, state)
                state, feedback = _handle_decision_result(state, decision)

                if decision.decision_type in (DecisionType.COMPLETE, DecisionType.FAIL):
                    break

        logger.info("workflow_completed",
                   session_id=state.session_id,
                   final_stage=state.current_stage.value)

        if state.final_output:
            return state.final_output
        elif state.error_message:
            return f"ì‹¤íŒ¨: {state.error_message}"
        else:
            return "ì›Œí¬í”Œë¡œìš°ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ì¶œë ¥ì´ ì—†ìŠµë‹ˆë‹¤."

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._client:
            await self._client.close()
