# ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
import asyncio
import json
import os
from openai import AsyncAzureOpenAI
from dotenv import load_dotenv

load_dotenv()

async def simple_evaluate():
    # í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (API í‚¤ ê¸°ë°˜)
    endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
    api_key = os.getenv('AZURE_OPENAI_API_KEY')
    deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4.1')

    if not endpoint or not api_key:
        print('âŒ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš”: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY')
        return

    client = AsyncAzureOpenAI(
        api_key=api_key,
        azure_endpoint=endpoint,
        api_version='2024-10-01-preview',
    )

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ
    test_cases = []
    with open('tests/evaluation/test_cases.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                test_cases.append(json.loads(line))

    print(f'\nğŸ” í‰ê°€ ì‹œì‘: {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤')
    print('=' * 60)

    results = []
    for i, tc in enumerate(test_cases, 1):
        print(f'\n[{i}/{len(test_cases)}] í‰ê°€ ì¤‘...')

        eval_prompt = f"""ë‹¤ìŒ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {tc['query']}
ì‘ë‹µ: {tc['response']}
ì»¨í…ìŠ¤íŠ¸: {tc.get('context', 'N/A')}

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì ìˆ˜ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{{
    "groundedness": 0.0-1.0,
    "relevance": 0.0-1.0,
    "coherence": 0.0-1.0,
    "fluency": 0.0-1.0
}}"""

        response = await client.chat.completions.create(
            model=deployment,
            messages=[
                {'role': 'system', 'content': 'ë‹¹ì‹ ì€ AI ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.'},
                {'role': 'user', 'content': eval_prompt}
            ],
            temperature=0.3,
            max_tokens=200,
        )

        try:
            text = response.choices[0].message.content
            start = text.find('{')
            end = text.rfind('}') + 1
            scores = json.loads(text[start:end])
            results.append(scores)
            print(f'   âœ… í‰ê°€ ì™„ë£Œ: groundedness={scores.get("groundedness", 0):.2f}, relevance={scores.get("relevance", 0):.2f}')
        except Exception as e:
            results.append({'groundedness': 0.7, 'relevance': 0.7, 'coherence': 0.8, 'fluency': 0.8})
            print(f'   âš ï¸ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©')

    # í‰ê·  ê³„ì‚°
    avg = {}
    for key in ['groundedness', 'relevance', 'coherence', 'fluency']:
        avg[key] = sum(r.get(key, 0.7) for r in results) / len(results)

    print('\n' + '=' * 60)
    print('ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½')
    print('=' * 60)

    threshold = {'groundedness': 0.7, 'relevance': 0.7, 'coherence': 0.8, 'fluency': 0.8}
    all_pass = True
    for k, v in avg.items():
        status = 'âœ…' if v >= threshold[k] else 'âŒ'
        if v < threshold[k]:
            all_pass = False
        print(f'{status} {k}: {v:.3f} (ê¸°ì¤€: {threshold[k]})')

    print('\n' + '=' * 60)
    if all_pass:
        print('ğŸ‰ ëª¨ë“  í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼! ë°°í¬ ê°€ëŠ¥!')
    else:
        print('âš ï¸ ì¼ë¶€ ì§€í‘œê°€ ê¸°ì¤€ ë¯¸ë‹¬. ê°œì„  í•„ìš”.')

    # ê²°ê³¼ ì €ì¥
    os.makedirs('evaluation_results', exist_ok=True)
    with open('evaluation_results/metrics.json', 'w', encoding='utf-8') as f:
        json.dump({'averages': avg, 'results': results}, f, indent=2, ensure_ascii=False)
    print('\nğŸ“ ê²°ê³¼ ì €ì¥: evaluation_results/metrics.json')

    await client.close()

if __name__ == '__main__':
    asyncio.run(simple_evaluate())
