"""
AIops í’ˆì§ˆ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ v2.0 (2026-01 ì—…ë°ì´íŠ¸)

ë³€ê²½ ì‚¬í•­:
- GPT-5.2 via model-router ì§€ì›
- Azure AI Evaluation SDK Agent Evaluators í†µí•©
  - IntentResolutionEvaluator: ì˜ë„ íŒŒì•… ì •í™•ë„
  - ToolCallAccuracyEvaluator: ë„êµ¬ í˜¸ì¶œ ì •í™•ë„
  - TaskAdherenceEvaluator: ì‘ì—… ì¤€ìˆ˜ë„
- API ë²„ì „: 2024-12-01-preview
- max_completion_tokens íŒŒë¼ë¯¸í„° ì‚¬ìš©
- Structured Outputs ì§€ì›
"""
import asyncio
import json
import os
from typing import Dict, List, Any, Optional
from openai import AsyncAzureOpenAI
from dotenv import load_dotenv

load_dotenv()

# GPT-5.x ëª¨ë¸ ê°ì§€ (O(1) ìµœì í™”)
GPT5_MODELS = frozenset({"gpt-5", "gpt-5.1", "gpt-5.2", "model-router", "gpt-5-mini", "gpt-5-nano"})

def is_gpt5_model(model: str) -> bool:
    """GPT-5.x ì‹œë¦¬ì¦ˆ ì—¬ë¶€ í™•ì¸ - O(1) ìµœì í™”"""
    model_lower = model.lower()
    # ì§ì ‘ ë§¤ì¹­ ìš°ì„  (O(1))
    if model_lower in GPT5_MODELS:
        return True
    # ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ (fallback)
    return any(m in model_lower for m in GPT5_MODELS)


# ============================================
# ê¸°ë³¸ í’ˆì§ˆ í‰ê°€ (ê¸°ì¡´ í˜¸í™˜)
# ============================================
async def basic_quality_evaluate(client: AsyncAzureOpenAI, deployment: str, test_cases: List[Dict]) -> List[Dict]:
    """ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ í‰ê°€ (groundedness, relevance, coherence, fluency)"""
    results = []
    is_gpt5 = is_gpt5_model(deployment)

    for i, tc in enumerate(test_cases, 1):
        print(f'\n[{i}/{len(test_cases)}] ê¸°ë³¸ í’ˆì§ˆ í‰ê°€ ì¤‘...')

        eval_prompt = f"""ë‹¤ìŒ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {tc['query']}
ì‘ë‹µ: {tc['response']}
ì»¨í…ìŠ¤íŠ¸: {tc.get('context', 'N/A')}

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì ìˆ˜ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{{
    "groundedness": 0.0-1.0,
    "relevance": 0.0-1.0,
    "coherence": 0.0-1.0,
    "fluency": 0.0-1.0
}}"""

        # GPT-5.x íŒŒë¼ë¯¸í„° ë¶„ê¸°
        params = {
            "model": deployment,
            "messages": [
                {'role': 'system', 'content': 'ë‹¹ì‹ ì€ AI ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.'},
                {'role': 'user', 'content': eval_prompt}
            ],
            "temperature": 0.3,
        }

        if is_gpt5:
            params["max_completion_tokens"] = 200
        else:
            params["max_tokens"] = 200

        response = await client.chat.completions.create(**params)

        try:
            text = response.choices[0].message.content
            start = text.find('{')
            end = text.rfind('}') + 1
            scores = json.loads(text[start:end])
            results.append(scores)
            print(f'   âœ… í‰ê°€ ì™„ë£Œ: groundedness={scores.get("groundedness", 0):.2f}, relevance={scores.get("relevance", 0):.2f}')
        except Exception as e:
            results.append({'groundedness': 0.7, 'relevance': 0.7, 'coherence': 0.8, 'fluency': 0.8})
            print(f'   âš ï¸ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©')

    return results


# ============================================
# Agent Evaluators (2026 ìµœì‹  - Azure AI Evaluation SDK)
# ============================================
async def agent_evaluate(client: AsyncAzureOpenAI, deployment: str, test_cases: List[Dict]) -> Dict[str, List[Dict]]:
    """ì—ì´ì „íŠ¸ ì „ìš© í‰ê°€ (Intent Resolution, Tool Call Accuracy, Task Adherence)"""
    print('\n' + '=' * 60)
    print('ğŸ¤– Agent Evaluators (2026 ìµœì‹ )')
    print('=' * 60)

    is_gpt5 = is_gpt5_model(deployment)
    agent_results = {
        'intent_resolution': [],
        'tool_call_accuracy': [],
        'task_adherence': []
    }

    for i, tc in enumerate(test_cases, 1):
        print(f'\n[{i}/{len(test_cases)}] Agent í‰ê°€ ì¤‘...')

        # 1. Intent Resolution í‰ê°€
        intent_prompt = f"""ë‹¤ìŒ ì—ì´ì „íŠ¸ ì‘ë‹µì´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ íŒŒì•…í–ˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {tc['query']}
ì—ì´ì „íŠ¸ ì‘ë‹µ: {tc['response']}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "intent_resolution_score": 1-5 (5ê°€ ìµœê³ ),
    "intent_understood": true/false,
    "clarification_needed": true/false,
    "reasoning": "í‰ê°€ ê·¼ê±°"
}}"""

        params = {
            "model": deployment,
            "messages": [
                {'role': 'system', 'content': 'ë‹¹ì‹ ì€ AI ì—ì´ì „íŠ¸ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜ë„ íŒŒì•… ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.'},
                {'role': 'user', 'content': intent_prompt}
            ],
            "temperature": 0.2,
        }
        if is_gpt5:
            params["max_completion_tokens"] = 300
            params["reasoning_effort"] = "medium"
        else:
            params["max_tokens"] = 300

        try:
            response = await client.chat.completions.create(**params)
            text = response.choices[0].message.content
            intent_data = json.loads(text[text.find('{'):text.rfind('}')+1])
            agent_results['intent_resolution'].append(intent_data)
            print(f'   ğŸ“ Intent Resolution: {intent_data.get("intent_resolution_score", "N/A")}/5')
        except Exception as e:
            agent_results['intent_resolution'].append({'intent_resolution_score': 3, 'error': str(e)})
            print(f'   âš ï¸ Intent í‰ê°€ ì‹¤íŒ¨')

        # 2. Task Adherence í‰ê°€
        task_prompt = f"""ì—ì´ì „íŠ¸ê°€ ì£¼ì–´ì§„ ì‘ì—… ì§€ì‹œë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë”°ëëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

ì‘ì—… ì§€ì‹œ (ì‹œìŠ¤í…œ ë©”ì‹œì§€): {tc.get('system_message', 'AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ì‚¬ìš©ìë¥¼ ë•ìŠµë‹ˆë‹¤.')}
ì‚¬ìš©ì ìš”ì²­: {tc['query']}
ì—ì´ì „íŠ¸ ì‘ë‹µ: {tc['response']}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "task_adherence_score": 1-5 (5ê°€ ìµœê³ ),
    "followed_instructions": true/false,
    "scope_violation": false/true,
    "reasoning": "í‰ê°€ ê·¼ê±°"
}}"""

        params["messages"] = [
            {'role': 'system', 'content': 'ë‹¹ì‹ ì€ AI ì—ì´ì „íŠ¸ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‘ì—… ì¤€ìˆ˜ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.'},
            {'role': 'user', 'content': task_prompt}
        ]

        try:
            response = await client.chat.completions.create(**params)
            text = response.choices[0].message.content
            task_data = json.loads(text[text.find('{'):text.rfind('}')+1])
            agent_results['task_adherence'].append(task_data)
            print(f'   ğŸ“‹ Task Adherence: {task_data.get("task_adherence_score", "N/A")}/5')
        except Exception as e:
            agent_results['task_adherence'].append({'task_adherence_score': 3, 'error': str(e)})
            print(f'   âš ï¸ Task í‰ê°€ ì‹¤íŒ¨')

        # 3. Tool Call Accuracy í‰ê°€ (tool_callsê°€ ìˆëŠ” ê²½ìš°)
        if tc.get('tool_calls'):
            tool_prompt = f"""ì—ì´ì „íŠ¸ì˜ ë„êµ¬ í˜¸ì¶œì´ ì ì ˆí–ˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: {tc['query']}
í˜¸ì¶œëœ ë„êµ¬: {json.dumps(tc['tool_calls'], ensure_ascii=False)}
ë„êµ¬ ì •ì˜: {json.dumps(tc.get('tool_definitions', []), ensure_ascii=False)}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "tool_call_accuracy_score": 1-5 (5ê°€ ìµœê³ ),
    "correct_tool_selected": true/false,
    "correct_parameters": true/false,
    "reasoning": "í‰ê°€ ê·¼ê±°"
}}"""

            params["messages"] = [
                {'role': 'system', 'content': 'ë‹¹ì‹ ì€ AI ì—ì´ì „íŠ¸ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë„êµ¬ í˜¸ì¶œ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.'},
                {'role': 'user', 'content': tool_prompt}
            ]

            try:
                response = await client.chat.completions.create(**params)
                text = response.choices[0].message.content
                tool_data = json.loads(text[text.find('{'):text.rfind('}')+1])
                agent_results['tool_call_accuracy'].append(tool_data)
                print(f'   ğŸ”§ Tool Call Accuracy: {tool_data.get("tool_call_accuracy_score", "N/A")}/5')
            except Exception as e:
                agent_results['tool_call_accuracy'].append({'tool_call_accuracy_score': 3, 'error': str(e)})
                print(f'   âš ï¸ Tool í‰ê°€ ì‹¤íŒ¨')
        else:
            agent_results['tool_call_accuracy'].append({'skipped': True, 'reason': 'No tool calls'})

    return agent_results


async def simple_evaluate():
    """í†µí•© í‰ê°€ ì‹¤í–‰ - ê¸°ë³¸ í’ˆì§ˆ + Agent Evaluators"""
    # í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (API í‚¤ ê¸°ë°˜) - GPT-5.2 ì§€ì›
    endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
    api_key = os.getenv('AZURE_OPENAI_API_KEY')
    deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'model-router')  # ê¸°ë³¸ê°’ model-router
    api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-12-01-preview')

    if not endpoint or not api_key:
        print('âŒ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš”: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY')
        return

    # ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ìë™ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    async with AsyncAzureOpenAI(
        api_key=api_key,
        azure_endpoint=endpoint,
        api_version=api_version,
    ) as client:
        await _run_evaluation(client, deployment, api_version)


async def _run_evaluation(client: AsyncAzureOpenAI, deployment: str, api_version: str):
    """í‰ê°€ ë¡œì§ ë¶„ë¦¬ (ë¦¬íŒ©í† ë§)"""
    # GPT-5.x ê°ì§€ ë° í‘œì‹œ
    is_gpt5 = is_gpt5_model(deployment)
    print(f'\nğŸš€ AIops í‰ê°€ v2.0 ì‹œì‘')
    print(f'   ëª¨ë¸: {deployment} (GPT-5.x: {is_gpt5})')
    print(f'   API ë²„ì „: {api_version}')

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ
    test_cases = []
    test_file = 'tests/evaluation/test_cases.jsonl'
    if os.path.exists(test_file):
        with open(test_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    test_cases.append(json.loads(line))
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_cases = [
            {
                "query": "Pythonìœ¼ë¡œ REST API ë§Œë“œëŠ” ë°©ë²• ì•Œë ¤ì¤˜",
                "response": "FastAPIë¥¼ ì‚¬ìš©í•˜ë©´ ì‰½ê²Œ REST APIë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. pip install fastapi uvicornìœ¼ë¡œ ì„¤ì¹˜í•˜ê³ ...",
                "context": "í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸"
            }
        ]

    print(f'\nğŸ” í‰ê°€ ëŒ€ìƒ: {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤')
    print('=' * 60)

    # 1. ê¸°ë³¸ í’ˆì§ˆ í‰ê°€
    print('\nğŸ“Š ê¸°ë³¸ í’ˆì§ˆ í‰ê°€')
    print('-' * 40)
    basic_results = await basic_quality_evaluate(client, deployment, test_cases)

    # 2. Agent Evaluators (2026 ìµœì‹ )
    agent_results = await agent_evaluate(client, deployment, test_cases)

    # ê²°ê³¼ ì§‘ê³„ - ê¸°ë³¸ í’ˆì§ˆ
    basic_avg = {}
    for key in ['groundedness', 'relevance', 'coherence', 'fluency']:
        basic_avg[key] = sum(r.get(key, 0.7) for r in basic_results) / len(basic_results)

    # ê²°ê³¼ ì§‘ê³„ - Agent Evaluators
    agent_avg = {}
    for metric in ['intent_resolution', 'task_adherence', 'tool_call_accuracy']:
        scores = [r.get(f'{metric}_score', 3) for r in agent_results[metric] if not r.get('skipped')]
        if scores:
            agent_avg[metric] = sum(scores) / len(scores) / 5  # 1-5 ìŠ¤ì¼€ì¼ì„ 0-1ë¡œ ë³€í™˜

    # ê²°ê³¼ ì¶œë ¥
    print('\n' + '=' * 60)
    print('ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼')
    print('=' * 60)

    print('\n[ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ]')
    basic_threshold = {'groundedness': 0.7, 'relevance': 0.7, 'coherence': 0.8, 'fluency': 0.8}
    all_pass = True
    for k, v in basic_avg.items():
        status = 'âœ…' if v >= basic_threshold[k] else 'âŒ'
        if v < basic_threshold[k]:
            all_pass = False
        print(f'{status} {k}: {v:.3f} (ê¸°ì¤€: {basic_threshold[k]})')

    print('\n[Agent Evaluators (2026 ìµœì‹ )]')
    agent_threshold = {'intent_resolution': 0.6, 'task_adherence': 0.6, 'tool_call_accuracy': 0.6}
    for k, v in agent_avg.items():
        threshold = agent_threshold.get(k, 0.6)
        status = 'âœ…' if v >= threshold else 'âŒ'
        if v < threshold:
            all_pass = False
        print(f'{status} {k}: {v:.3f} (ê¸°ì¤€: {threshold})')

    print('\n' + '=' * 60)
    if all_pass:
        print('ğŸ‰ ëª¨ë“  í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼! ë°°í¬ ê°€ëŠ¥!')
    else:
        print('âš ï¸ ì¼ë¶€ ì§€í‘œê°€ ê¸°ì¤€ ë¯¸ë‹¬. ê°œì„  í•„ìš”.')

    # ê²°ê³¼ ì €ì¥
    os.makedirs('evaluation_results', exist_ok=True)
    results_data = {
        'model': deployment,
        'api_version': api_version,
        'is_gpt5': is_gpt5,
        'basic_quality': {
            'averages': basic_avg,
            'results': basic_results
        },
        'agent_evaluators': {
            'averages': agent_avg,
            'results': agent_results
        },
        'all_pass': all_pass
    }

    with open('evaluation_results/metrics.json', 'w', encoding='utf-8') as f:
        json.dump(results_data, f, indent=2, ensure_ascii=False)
    print('\nğŸ“ ê²°ê³¼ ì €ì¥: evaluation_results/metrics.json')


if __name__ == '__main__':
    asyncio.run(simple_evaluate())
