# Copyright (c) Microsoft. All rights reserved.
"""
Agent Quality Evaluation Runner
Azure AI Evaluation SDKë¥¼ ì‚¬ìš©í•œ ì—ì´ì „íŠ¸ í’ˆì§ˆ í‰ê°€

100% Azure ê¸°ë°˜:
- Azure AI Evaluation SDK (í‰ê°€)
- Azure AI Foundry (ëª¨ë¸)
- Azure Application Insights (ê²°ê³¼ ë¡œê¹…)
- Azure Blob Storage (ê²°ê³¼ ì €ì¥)

ìµœì í™”:
- ë³‘ë ¬ í‰ê°€ ì²˜ë¦¬ (asyncio.gather)
- ì—°ê²° í’€ë§ ë° ì¬ì‚¬ìš©
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
"""

from __future__ import annotations

import argparse
import asyncio
import json
import os
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timezone
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# Azure SDK imports
from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient
from azure.storage.blob.aio import BlobServiceClient as AsyncBlobServiceClient

# Azure AI Evaluation imports
try:
    from azure.ai.evaluation import (
        GroundednessEvaluator,
        RelevanceEvaluator,
        CoherenceEvaluator,
        FluencyEvaluator,
        SimilarityEvaluator,
        F1ScoreEvaluator,
        evaluate,
    )
    AZURE_EVAL_AVAILABLE = True
except ImportError:
    AZURE_EVAL_AVAILABLE = False
    print("Warning: azure-ai-evaluation not installed. Run: pip install azure-ai-evaluation")

# Azure Monitor for logging
try:
    from azure.monitor.opentelemetry import configure_azure_monitor
    from opentelemetry import trace
    AZURE_MONITOR_AVAILABLE = True
except ImportError:
    AZURE_MONITOR_AVAILABLE = False

# ìƒìˆ˜ ì •ì˜
DEFAULT_METRICS = ("groundedness", "relevance", "coherence", "fluency")
MAX_CONCURRENT_EVALUATIONS = 5
QUALITY_THRESHOLD = 0.7


class AzureEvaluationRunner:
    """Azure AI Evaluation SDK ê¸°ë°˜ ì—ì´ì „íŠ¸ í‰ê°€ ì‹¤í–‰ê¸° (ìµœì í™” ë²„ì „)"""

    __slots__ = (
        'credential', 'project_endpoint', 'model_deployment',
        'storage_connection', 'storage_container', 'app_insights_connection',
        'evaluators', 'tracer', '_semaphore', '_executor'
    )

    def __init__(
        self,
        azure_ai_project_endpoint: Optional[str] = None,
        model_deployment: Optional[str] = None,
        storage_connection_string: Optional[str] = None,
        app_insights_connection_string: Optional[str] = None,
        max_concurrent: int = MAX_CONCURRENT_EVALUATIONS,
    ):
        # Azure ìê²© ì¦ëª… (ì‹±ê¸€í†¤ íŒ¨í„´)
        self.credential = DefaultAzureCredential()

        # Azure AI Foundry ì„¤ì •
        self.project_endpoint = azure_ai_project_endpoint or os.getenv("AZURE_FOUNDRY_PROJECT_ENDPOINT")
        self.model_deployment = model_deployment or os.getenv("AZURE_FOUNDRY_MODEL_DEPLOYMENT", "gpt-4.1")

        # Azure Storage ì„¤ì • (ê²°ê³¼ ì €ì¥ìš©)
        self.storage_connection = storage_connection_string or os.getenv("AZURE_STORAGE_CONNECTION_STRING")
        self.storage_container = os.getenv("AZURE_EVAL_RESULTS_CONTAINER", "evaluation-results")

        # Azure Application Insights ì„¤ì • (ëª¨ë‹ˆí„°ë§)
        self.app_insights_connection = app_insights_connection_string or os.getenv("APPLICATIONINSIGHTS_CONNECTION_STRING")

        # ë™ì‹œì„± ì œì–´
        self._semaphore = asyncio.Semaphore(max_concurrent)
        self._executor = ThreadPoolExecutor(max_workers=max_concurrent)

        # í‰ê°€ê¸° ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self.evaluators = self._initialize_azure_evaluators()

        # Azure Monitor ì´ˆê¸°í™”
        self._setup_azure_monitor()

    def _setup_azure_monitor(self) -> None:
        """Azure Application Insights ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        if AZURE_MONITOR_AVAILABLE and self.app_insights_connection:
            try:
                configure_azure_monitor(
                    connection_string=self.app_insights_connection,
                    enable_live_metrics=True,
                )
                self.tracer = trace.get_tracer(__name__)
                print("âœ… Azure Application Insights ì—°ê²°ë¨")
            except Exception as e:
                print(f"Warning: Azure Monitor ì„¤ì • ì‹¤íŒ¨: {e}")
                self.tracer = None
        else:
            self.tracer = None

    def _initialize_azure_evaluators(self) -> Dict[str, Any]:
        """Azure AI Evaluation SDK í‰ê°€ê¸° ì´ˆê¸°í™”"""
        if not AZURE_EVAL_AVAILABLE:
            raise ImportError("azure-ai-evaluation SDKê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install azure-ai-evaluation")

        if not self.project_endpoint:
            raise ValueError("AZURE_FOUNDRY_PROJECT_ENDPOINT í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

        # Azure AI Foundry ëª¨ë¸ ì„¤ì •
        model_config = {
            "azure_endpoint": self.project_endpoint,
            "azure_deployment": self.model_deployment,
            "api_version": "2024-10-01-preview",
        }

        print(f"âœ… Azure AI Foundry ì—°ê²°: {self.project_endpoint}")
        print(f"   ëª¨ë¸ ë°°í¬: {self.model_deployment}")

        return {
            # í’ˆì§ˆ í‰ê°€ ì§€í‘œ
            "groundedness": GroundednessEvaluator(model_config=model_config),
            "relevance": RelevanceEvaluator(model_config=model_config),
            "coherence": CoherenceEvaluator(model_config=model_config),
            "fluency": FluencyEvaluator(model_config=model_config),
            # ìœ ì‚¬ë„ í‰ê°€
            "similarity": SimilarityEvaluator(model_config=model_config),
            # F1 ì ìˆ˜ (ì •ë‹µì´ ìˆëŠ” ê²½ìš°)
            "f1_score": F1ScoreEvaluator(),
        }

    async def _evaluate_metric(
        self,
        metric: str,
        query: str,
        response: str,
        context: Optional[str],
        ground_truth: Optional[str],
    ) -> Tuple[str, float]:
        """ë‹¨ì¼ ë©”íŠ¸ë¦­ í‰ê°€ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        if metric not in self.evaluators:
            return metric, 0.0

        async with self._semaphore:
            try:
                evaluator = self.evaluators[metric]

                # ThreadPoolì—ì„œ ë™ê¸° í‰ê°€ê¸° ì‹¤í–‰ (ë¸”ë¡œí‚¹ ë°©ì§€)
                loop = asyncio.get_event_loop()

                if metric == "groundedness" and context:
                    eval_result = await loop.run_in_executor(
                        self._executor,
                        lambda: evaluator(query=query, response=response, context=context)
                    )
                elif metric == "f1_score" and ground_truth:
                    eval_result = await loop.run_in_executor(
                        self._executor,
                        lambda: evaluator(response=response, ground_truth=ground_truth)
                    )
                elif metric == "similarity" and ground_truth:
                    eval_result = await loop.run_in_executor(
                        self._executor,
                        lambda: evaluator(response=response, ground_truth=ground_truth)
                    )
                else:
                    eval_result = await loop.run_in_executor(
                        self._executor,
                        lambda: evaluator(query=query, response=response)
                    )

                score_key = f"gpt_{metric}"
                score = eval_result.get(score_key, eval_result.get(metric, 0.0))
                return metric, float(score)

            except Exception as e:
                print(f"âš ï¸ {metric} í‰ê°€ ì‹¤íŒ¨: {e}")
                return metric, 0.0

    async def evaluate_single(
        self,
        query: str,
        response: str,
        context: Optional[str] = None,
        ground_truth: Optional[str] = None,
        metrics: Optional[List[str]] = None,
    ) -> Dict[str, float]:
        """ë‹¨ì¼ ì‘ë‹µ í‰ê°€ (ë³‘ë ¬ ë©”íŠ¸ë¦­ í‰ê°€)"""
        metrics = metrics or list(DEFAULT_METRICS)

        # íŠ¸ë ˆì´ì‹± ì‹œì‘
        span_context = None
        if self.tracer:
            span_context = self.tracer.start_span("evaluate_single")
            span_context.set_attribute("query_length", len(query))
            span_context.set_attribute("metrics", ",".join(metrics))

        # ëª¨ë“  ë©”íŠ¸ë¦­ì„ ë³‘ë ¬ë¡œ í‰ê°€
        tasks = [
            self._evaluate_metric(metric, query, response, context, ground_truth)
            for metric in metrics
        ]

        metric_results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì§‘ê³„
        results: Dict[str, float] = {}
        for result in metric_results:
            if isinstance(result, tuple):
                metric_name, score = result
                results[metric_name] = score
            elif isinstance(result, Exception):
                print(f"âš ï¸ í‰ê°€ ì˜ˆì™¸ ë°œìƒ: {result}")

        if span_context:
            span_context.end()

        return results

    async def _evaluate_item(
        self,
        index: int,
        item: Dict[str, Any],
        metrics: Optional[List[str]],
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì•„ì´í…œ í‰ê°€ (ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        result = await self.evaluate_single(
            query=item.get("query", item.get("question", "")),
            response=item.get("response", item.get("answer", "")),
            context=item.get("context"),
            ground_truth=item.get("ground_truth"),
            metrics=metrics,
        )
        result["index"] = index
        result["query"] = item.get("query", item.get("question", ""))[:100]
        return result

    async def evaluate_batch(
        self,
        test_data_path: str,
        output_dir: str,
        metrics: Optional[List[str]] = None,
        upload_to_azure: bool = True,
        batch_size: int = 10,
    ) -> Dict[str, Any]:
        """ë°°ì¹˜ í‰ê°€ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”)"""
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_data = self._load_test_data(test_data_path)

        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        all_results: List[Dict[str, Any]] = []
        metric_sums: Dict[str, float] = defaultdict(float)
        num_items = len(test_data)

        print(f"\nğŸ” í‰ê°€ ì‹œì‘: {num_items}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
        print(f"   í‰ê°€ ì§€í‘œ: {metrics or 'all'}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ì²˜ë¦¬
        for batch_start in range(0, num_items, batch_size):
            batch_end = min(batch_start + batch_size, num_items)
            batch = test_data[batch_start:batch_end]

            print(f"  [{batch_start+1}-{batch_end}/{num_items}] í‰ê°€ ì¤‘...")

            # ë°°ì¹˜ ë‚´ ì•„ì´í…œë“¤ì„ ë³‘ë ¬ë¡œ í‰ê°€
            tasks = [
                self._evaluate_item(batch_start + i, item, metrics)
                for i, item in enumerate(batch)
            ]

            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in batch_results:
                if isinstance(result, dict):
                    all_results.append(result)
                    for key, score in result.items():
                        if isinstance(score, (int, float)):
                            metric_sums[key] += score
                elif isinstance(result, Exception):
                    print(f"  âš ï¸ ë°°ì¹˜ í‰ê°€ ì˜ˆì™¸: {result}")

        # í‰ê·  ê³„ì‚°
        averages = {
            metric: round(total / num_items, 3)
            for metric, total in metric_sums.items()
            if metric not in ("index",)
        }

        # ê²°ê³¼ ìš”ì•½
        summary = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "azure_project_endpoint": self.project_endpoint,
            "model_deployment": self.model_deployment,
            "num_items": num_items,
            "averages": averages,
            "quality_passed": all(
                avg >= QUALITY_THRESHOLD
                for key, avg in averages.items()
                if key in DEFAULT_METRICS
            ),
            "results": all_results,
        }

        # ë¡œì»¬ ì €ì¥ (ë¹„ë™ê¸° íŒŒì¼ I/O)
        await self._save_results_async(summary, all_results, output_path)

        # Azure Blob Storageì— ì—…ë¡œë“œ
        if upload_to_azure and self.storage_connection:
            await self._upload_to_azure_storage(summary, output_path)

        # ê²°ê³¼ ì¶œë ¥
        self._print_summary(num_items, averages)

        return summary

    async def _save_results_async(
        self,
        summary: Dict[str, Any],
        results: List[Dict[str, Any]],
        output_path: Path,
    ) -> None:
        """ë¹„ë™ê¸° ê²°ê³¼ ì €ì¥"""
        loop = asyncio.get_event_loop()

        # metrics.json ì €ì¥
        metrics_file = output_path / "metrics.json"
        await loop.run_in_executor(
            None,
            lambda: metrics_file.write_text(
                json.dumps(summary, indent=2, ensure_ascii=False),
                encoding="utf-8"
            )
        )

        # detailed_results.jsonl ì €ì¥
        detailed_file = output_path / "detailed_results.jsonl"
        lines = [json.dumps(r, ensure_ascii=False) for r in results]
        await loop.run_in_executor(
            None,
            lambda: detailed_file.write_text("\n".join(lines), encoding="utf-8")
        )

    @staticmethod
    def _print_summary(num_items: int, averages: Dict[str, float]) -> None:
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
        print(f"   ì´ í•­ëª©: {num_items}")
        for metric, avg in averages.items():
            if metric == "index":
                continue
            status = "âœ…" if avg >= QUALITY_THRESHOLD else "âš ï¸"
            print(f"   {status} {metric}: {avg}")

    async def _upload_to_azure_storage(self, summary: Dict, output_path: Path) -> None:
        """Azure Blob Storageì— ê²°ê³¼ ì—…ë¡œë“œ (ë¹„ë™ê¸°)"""
        try:
            # ë¹„ë™ê¸° Blob í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
            async with AsyncBlobServiceClient.from_connection_string(
                self.storage_connection
            ) as blob_service:
                container_client = blob_service.get_container_client(self.storage_container)

                # ì»¨í…Œì´ë„ˆ ìƒì„± (ì—†ëŠ” ê²½ìš°)
                try:
                    await container_client.create_container()
                except Exception:
                    pass  # ì´ë¯¸ ì¡´ì¬

                # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê²½ë¡œ
                timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")

                # ë³‘ë ¬ ì—…ë¡œë“œ
                async def upload_file(filename: str) -> None:
                    blob_client = container_client.get_blob_client(
                        f"evaluations/{timestamp}/{filename}"
                    )
                    file_path = output_path / filename
                    async with blob_client:
                        with open(file_path, "rb") as f:
                            await blob_client.upload_blob(f, overwrite=True)

                await asyncio.gather(
                    upload_file("metrics.json"),
                    upload_file("detailed_results.jsonl"),
                )

                print(f"   â˜ï¸ Azure Blob Storage ì—…ë¡œë“œ ì™„ë£Œ: evaluations/{timestamp}/")

        except Exception as e:
            print(f"   âš ï¸ Azure Storage ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _load_test_data(self, path: str) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        test_data = []

        with open(path, "r", encoding="utf-8") as f:
            if path.endswith(".jsonl"):
                for line in f:
                    if line.strip():
                        test_data.append(json.loads(line))
            else:
                test_data = json.load(f)

        return test_data


def main():
    parser = argparse.ArgumentParser(
        description="Azure AI Evaluation SDK ê¸°ë°˜ AI ì—ì´ì „íŠ¸ í‰ê°€"
    )
    parser.add_argument(
        "--test-data",
        type=str,
        required=True,
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (JSONL ë˜ëŠ” JSON)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="evaluation_results",
        help="í‰ê°€ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "--metrics",
        type=str,
        nargs="+",
        default=["groundedness", "relevance", "coherence", "fluency"],
        help="í‰ê°€í•  ì§€í‘œ ëª©ë¡",
    )
    parser.add_argument(
        "--azure-project-endpoint",
        type=str,
        default=None,
        help="Azure AI Foundry í”„ë¡œì íŠ¸ ì—”ë“œí¬ì¸íŠ¸",
    )
    parser.add_argument(
        "--no-upload",
        action="store_true",
        help="Azure Storage ì—…ë¡œë“œ ë¹„í™œì„±í™”",
    )

    args = parser.parse_args()

    runner = AzureEvaluationRunner(
        azure_ai_project_endpoint=args.azure_project_endpoint,
    )

    asyncio.run(runner.evaluate_batch(
        test_data_path=args.test_data,
        output_dir=args.output_dir,
        metrics=args.metrics,
        upload_to_azure=not args.no_upload,
    ))


if __name__ == "__main__":
    main()
