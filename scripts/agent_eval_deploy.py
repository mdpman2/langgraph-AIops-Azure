#!/usr/bin/env python3
"""
Agent ìƒì„± â†’ í‰ê°€ â†’ Human-in-the-Loop Azure ë°°í¬ ì›Œí¬í”Œë¡œìš°

ì‚¬ìš©ë²•:
    python scripts/agent_eval_deploy.py                    # ì „ì²´ ì›Œí¬í”Œë¡œìš°
    python scripts/agent_eval_deploy.py --skip-agent       # í‰ê°€ + ë°°í¬ë§Œ
    python scripts/agent_eval_deploy.py --eval-only        # í‰ê°€ë§Œ
"""

import asyncio
import argparse
import json
import os
import sys
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT / 'src'))

from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / '.env')

# ============================================
# ì„¤ì •
# ============================================
EVALUATION_THRESHOLDS = {
    'groundedness': 0.7,
    'relevance': 0.7,
    'coherence': 0.8,
}

TEST_QUERIES = [
    "Pythonìœ¼ë¡œ ê°„ë‹¨í•œ REST API ì„œë²„ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "Azure Container Appsì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "LangGraph ìŠ¤íƒ€ì¼ì˜ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë€ ë¬´ì—‡ì¸ê°€ìš”?",
]


# ============================================
# Agent ì‹¤í–‰
# ============================================
async def run_agent(query: str) -> Dict:
    """ì—ì´ì „íŠ¸ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ìˆ˜ì§‘"""
    from langgraph_agent.workflow import AgentWorkflow

    workflow = AgentWorkflow()

    try:
        # AgentWorkflow.run()ì€ ë¬¸ìì—´ì„ ë°›ì•„ ë¬¸ìì—´ì„ ë°˜í™˜
        response = await workflow.run(query)

        return {
            'query': query,
            'response': response,
            'context': "Agent ì‹¤í–‰ ì™„ë£Œ",
        }
    finally:
        await workflow.close()


async def collect_agent_responses(queries: List[str]) -> List[Dict]:
    """ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•œ ì—ì´ì „íŠ¸ ì‘ë‹µ ìˆ˜ì§‘"""
    print("\n" + "=" * 60)
    print("ğŸ¤– STAGE 1: Agent ì‘ë‹µ ìˆ˜ì§‘")
    print("=" * 60)

    results = []
    for i, query in enumerate(queries, 1):
        print(f"\n[{i}/{len(queries)}] ì¿¼ë¦¬: {query[:50]}...")
        try:
            result = await run_agent(query)
            results.append(result)
            print(f"   âœ… ì‘ë‹µ ìˆ˜ì§‘ ì™„ë£Œ ({len(result['response'])} ì)")
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            results.append({
                'query': query,
                'response': f"ì˜¤ë¥˜ ë°œìƒ: {e}",
                'context': "Agent ì‹¤í–‰ ì‹¤íŒ¨"
            })

    # ì‘ë‹µ ì €ì¥
    output_file = PROJECT_ROOT / 'evaluation_results' / 'agent_responses.jsonl'
    output_file.parent.mkdir(exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + '\n')

    print(f"\nğŸ“ ì‘ë‹µ ì €ì¥ë¨: {output_file}")
    return results


# ============================================
# í‰ê°€
# ============================================
async def evaluate_responses(responses: List[Dict]) -> Tuple[Dict, bool]:
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    from openai import AsyncAzureOpenAI

    print("\n" + "=" * 60)
    print("ğŸ“Š STAGE 2: í’ˆì§ˆ í‰ê°€")
    print("=" * 60)

    endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
    api_key = os.getenv('AZURE_OPENAI_API_KEY')
    deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4.1')

    if not endpoint or not api_key:
        print("âŒ Azure OpenAI ì„¤ì • í•„ìš”")
        return {}, False

    client = AsyncAzureOpenAI(
        api_key=api_key,
        azure_endpoint=endpoint,
        api_version='2024-10-01-preview',
    )

    all_scores = []
    for i, item in enumerate(responses, 1):
        print(f"\n[{i}/{len(responses)}] í‰ê°€ ì¤‘...")

        eval_prompt = f"""ë‹¤ìŒ AI ì—ì´ì „íŠ¸ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {item['query']}
ì‘ë‹µ: {item['response']}
ì»¨í…ìŠ¤íŠ¸: {item.get('context', 'N/A')}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ 0.0~1.0 ì‚¬ì´ ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ì„¸ìš”:
{{
    "groundedness": <ì‘ë‹µì´ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ëŠ” ì •ë„>,
    "relevance": <ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±>,
    "coherence": <ë…¼ë¦¬ì  ì¼ê´€ì„±>
}}"""

        try:
            response = await client.chat.completions.create(
                model=deployment,
                messages=[
                    {'role': 'system', 'content': 'ë‹¹ì‹ ì€ AI ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.'},
                    {'role': 'user', 'content': eval_prompt}
                ],
                temperature=0.3,
                max_tokens=200,
            )

            text = response.choices[0].message.content
            start = text.find('{')
            end = text.rfind('}') + 1
            scores = json.loads(text[start:end])
            all_scores.append(scores)
            print(f"   âœ… groundedness={scores.get('groundedness', 0):.2f}, "
                  f"relevance={scores.get('relevance', 0):.2f}, "
                  f"coherence={scores.get('coherence', 0):.2f}")
        except Exception as e:
            print(f"   âš ï¸ í‰ê°€ ì˜¤ë¥˜, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            all_scores.append({'groundedness': 0.7, 'relevance': 0.7, 'coherence': 0.8})

    # í‰ê·  ê³„ì‚°
    avg_scores = {}
    for key in ['groundedness', 'relevance', 'coherence']:
        avg_scores[key] = sum(s.get(key, 0.7) for s in all_scores) / len(all_scores)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "-" * 60)
    print("ğŸ“ˆ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("-" * 60)

    all_pass = True
    for metric, score in avg_scores.items():
        threshold = EVALUATION_THRESHOLDS.get(metric, 0.7)
        status = "âœ…" if score >= threshold else "âŒ"
        if score < threshold:
            all_pass = False
        print(f"   {status} {metric}: {score:.3f} (ê¸°ì¤€: {threshold})")

    print("-" * 60)

    # ê²°ê³¼ ì €ì¥
    result_file = PROJECT_ROOT / 'evaluation_results' / 'eval_summary.json'
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'scores': avg_scores,
            'thresholds': EVALUATION_THRESHOLDS,
            'all_pass': all_pass,
            'detail_scores': all_scores
        }, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“ í‰ê°€ ê²°ê³¼ ì €ì¥ë¨: {result_file}")

    return avg_scores, all_pass


# ============================================
# Human-in-the-Loop ë°°í¬ ê²°ì •
# ============================================
def human_deploy_decision(scores: Dict, all_pass: bool) -> Optional[str]:
    """ì‚¬ëŒì´ ë°°í¬ ì—¬ë¶€ ê²°ì •"""
    print("\n" + "=" * 60)
    print("ğŸ§‘â€ğŸ’» STAGE 3: ë°°í¬ ê²°ì • (Human-in-the-Loop)")
    print("=" * 60)

    if all_pass:
        print("\nğŸ‰ ëª¨ë“  í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼!")
        print("   ë°°í¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ì¼ë¶€ í’ˆì§ˆ ì§€í‘œê°€ ê¸°ì¤€ ë¯¸ë‹¬ì…ë‹ˆë‹¤.")
        print("   ë°°í¬ë¥¼ ì§„í–‰í•˜ë ¤ë©´ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    print("\n" + "-" * 60)
    print("ğŸ“Š í˜„ì¬ í’ˆì§ˆ ì ìˆ˜:")
    for metric, score in scores.items():
        threshold = EVALUATION_THRESHOLDS.get(metric, 0.7)
        status = "âœ…" if score >= threshold else "âš ï¸"
        print(f"   {status} {metric}: {score:.3f}")
    print("-" * 60)

    print("\nğŸš€ Azure ë°°í¬ ì˜µì…˜:")
    print("   1. ê¸°ì¡´ Azure OpenAI ì‚¬ìš© (deploy.py)")
    print("   2. ìƒˆ Azure OpenAI ìƒì„± (deploy_to_azure.py)")
    print("   0. ë°°í¬ ì•ˆ í•¨ (ì¢…ë£Œ)")
    print("-" * 60)

    while True:
        choice = input("\në°°í¬ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš” (0-2): ").strip()

        if choice == '0':
            print("\nâŒ ë°°í¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return None
        elif choice == '1':
            return 'existing'
        elif choice == '2':
            return 'new'
        else:
            print("âš ï¸ ì˜¬ë°”ë¥¸ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš” (0, 1, 2)")


def run_deployment(deploy_type: str, max_retries: int = 2) -> bool:
    """ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    print("\n" + "=" * 60)
    print("â˜ï¸ STAGE 4: Azure ë°°í¬")
    print("=" * 60)

    if deploy_type == 'existing':
        script = SCRIPT_DIR / 'deploy.py'
        print("\nğŸ“¦ ê¸°ì¡´ Azure OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°í¬í•©ë‹ˆë‹¤...")
    else:
        script = SCRIPT_DIR / 'deploy_to_azure.py'
        print("\nğŸ“¦ ìƒˆ Azure OpenAIë¥¼ ìƒì„±í•˜ì—¬ ë°°í¬í•©ë‹ˆë‹¤...")

    # ëŒ€í™”í˜• ëª¨ë“œë¡œ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§)
    for attempt in range(1, max_retries + 1):
        try:
            print(f"\nğŸ”„ ë°°í¬ ì‹œë„ {attempt}/{max_retries}...")

            result = subprocess.run(
                [sys.executable, str(script), '--interactive'],
                cwd=str(PROJECT_ROOT),
                shell=True
            )

            if result.returncode == 0:
                print(f"\nâœ… ë°°í¬ ì„±ê³µ!")
                return True
            else:
                print(f"\nâš ï¸ ë°°í¬ ë°˜í™˜ ì½”ë“œ: {result.returncode}")

                if attempt < max_retries:
                    retry = input("\nì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                    if retry != 'y':
                        print("ë°°í¬ë¥¼ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                        return False

        except KeyboardInterrupt:
            print("\n\nâŒ ì‚¬ìš©ìê°€ ë°°í¬ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ë°°í¬ ì˜¤ë¥˜: {e}")
            if attempt < max_retries:
                retry = input("\nì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                if retry != 'y':
                    return False

    print(f"\nâŒ ë°°í¬ ì‹¤íŒ¨ ({max_retries}íšŒ ì‹œë„)")
    return False


# ============================================
# ë©”ì¸ ì›Œí¬í”Œë¡œìš°
# ============================================
async def main_workflow(skip_agent: bool = False, eval_only: bool = False):
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("ğŸ”„ LangGraph Agent - í‰ê°€ & ë°°í¬ ì›Œí¬í”Œë¡œìš°")
    print("=" * 60)
    print(f"   ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # 1. Agent ì‘ë‹µ ìˆ˜ì§‘
    if skip_agent:
        print("\nâ­ï¸ Agent ì‹¤í–‰ ê±´ë„ˆëœ€, ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‚¬ìš©")
        responses = []
        test_file = PROJECT_ROOT / 'tests' / 'evaluation' / 'test_cases.jsonl'
        with open(test_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    responses.append(json.loads(line))
    else:
        responses = await collect_agent_responses(TEST_QUERIES)

    # 2. í‰ê°€
    scores, all_pass = await evaluate_responses(responses)

    if eval_only:
        print("\n" + "=" * 60)
        print("âœ… í‰ê°€ ì™„ë£Œ (ë°°í¬ ê±´ë„ˆëœ€)")
        print("=" * 60)
        return

    # 3. Human-in-the-Loop ë°°í¬ ê²°ì •
    deploy_type = human_deploy_decision(scores, all_pass)

    if deploy_type is None:
        return

    # 4. ë°°í¬ ì‹¤í–‰
    success = run_deployment(deploy_type)

    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 60)
    print(f"ğŸ“Š ì›Œí¬í”Œë¡œìš° ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"   í‰ê°€ ì ìˆ˜:")
    for metric, score in scores.items():
        threshold = EVALUATION_THRESHOLDS.get(metric, 0.7)
        status = "âœ…" if score >= threshold else "âš ï¸"
        print(f"     {status} {metric}: {score:.3f}")
    print(f"   í’ˆì§ˆ ê²Œì´íŠ¸: {'âœ… í†µê³¼' if all_pass else 'âš ï¸ ë¯¸ë‹¬'}")
    print(f"   ë°°í¬ ê²°ê³¼: {'âœ… ì„±ê³µ' if success else 'âŒ ì‹¤íŒ¨'}")
    print("=" * 60)

    if success:
        print("ğŸ‰ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!")
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. Azure Portalì—ì„œ ë¦¬ì†ŒìŠ¤ í™•ì¸")
        print("   2. Application Insightsì—ì„œ ëª¨ë‹ˆí„°ë§ í™•ì¸")
        print("   3. ì• í”Œë¦¬ì¼€ì´ì…˜ URLë¡œ ì ‘ì†í•˜ì—¬ í…ŒìŠ¤íŠ¸")
    else:
        print("âš ï¸ ë°°í¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("\nğŸ’¡ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("   1. Azure Portalì—ì„œ ë¡œê·¸ í™•ì¸")
        print("   2. 'az containerapp logs show' ëª…ë ¹ìœ¼ë¡œ ë¡œê·¸ í™•ì¸")
        print("   3. ë„¤íŠ¸ì›Œí¬/ê¶Œí•œ ì„¤ì • í™•ì¸")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description='Agent ìƒì„± â†’ í‰ê°€ â†’ Azure ë°°í¬ ì›Œí¬í”Œë¡œìš°'
    )
    parser.add_argument(
        '--skip-agent', '-s',
        action='store_true',
        help='Agent ì‹¤í–‰ ê±´ë„ˆë›°ê³  ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ í‰ê°€'
    )
    parser.add_argument(
        '--eval-only', '-e',
        action='store_true',
        help='í‰ê°€ë§Œ ì‹¤í–‰ (ë°°í¬ ê±´ë„ˆëœ€)'
    )
    parser.add_argument(
        '--queries', '-q',
        nargs='+',
        help='ì‚¬ìš©ì ì •ì˜ ì¿¼ë¦¬ ëª©ë¡'
    )

    args = parser.parse_args()

    if args.queries:
        global TEST_QUERIES
        TEST_QUERIES = args.queries

    asyncio.run(main_workflow(
        skip_agent=args.skip_agent,
        eval_only=args.eval_only
    ))


if __name__ == '__main__':
    main()
